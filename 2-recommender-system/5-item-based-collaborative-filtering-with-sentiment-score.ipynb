{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90395822-f83e-4b00-8644-39e2f265580d",
   "metadata": {},
   "source": [
    "# Sentiment based multi-index integrated scoring method to improve the accuracy of recommender system\n",
    "reference: https://www.sciencedirect.com/science/article/pii/S0957417421005467#b0230\n",
    "4.3. Multi-index integrated scoring method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b264e3-ddba-4722-a88d-0a1b882554dc",
   "metadata": {},
   "source": [
    "1. **Measure User Consistency per Aspect**\r\n",
    "\r\n",
    "    * For each user $u$ and aspect $a$, compute the Pearson correlation\r\n",
    "\r\n",
    "      $$\r\n",
    "      C_{u,a} = \\mathrm{Pearson}(\\{r_{u,i,a}\\},\\{s_{u,i,a}\\}).\r\n",
    "      $$  demonstrates more reliably on that aspect, all without any de-noising step.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa417c-574a-44fd-9335-0d23883f4f2d",
   "metadata": {},
   "source": [
    "2. **Derive an Adaptive Weight**\n",
    "\n",
    "   * Define\n",
    "\n",
    "     $$\n",
    "       w_{u,i,a} = 1 - C'_{u,a}.\n",
    "     $$\n",
    "   * Interpretation:\n",
    "\n",
    "     * **High consistency** ($C'\\approx1$) → $w\\approx0$ → default to the explicit rating.\n",
    "     * **Low consistency** → $w$ larger → rely more on sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd842ed-d640-4540-9e3f-da4b01b3aff4",
   "metadata": {},
   "source": [
    "3. **Fuse Rating & Sentiment per Aspect**\n",
    "\n",
    "   * Compute the final, blended score:\n",
    "\n",
    "     $$\n",
    "       r^*_{u,i,a}\n",
    "       = w_{u,i,a}\\,s_{u,i,a}\n",
    "       + (1 - w_{u,i,a})\\,r_{u,i,a}.\n",
    "     $$\n",
    "   * Each aspect gets its own adaptive mix of numeric rating and text-derived sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08252477-c1d8-4f3b-9cc7-51a4a27d1fc7",
   "metadata": {},
   "source": [
    "4. **Integrate into Recommendation**\n",
    "\n",
    "   * Treat the fused vector $\\{r^*_{u,i,1},\\dots,r^*_{u,i,A}\\}$ as the user’s multi-aspect preference.\n",
    "   * Plug this vector into your downstream model (e.g. multi-task matrix factorization or neural recommender).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper Functions",
   "id": "6545695feb77778e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:27.425298Z",
     "start_time": "2025-05-16T20:42:27.419024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_k(predictions, k=10):\n",
    "    '''Return the top-K recommended items for each user from predictions.'''\n",
    "    top_k = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_k[uid].append((iid, est))\n",
    "\n",
    "    # Sort and pick top k\n",
    "    for uid, user_ratings in top_k.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_k[uid] = [iid for (iid, _) in user_ratings[:k]]\n",
    "\n",
    "    return top_k"
   ],
   "id": "fd074b9ee2a83fa4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:27.606142Z",
     "start_time": "2025-05-16T20:42:27.601552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_true_positives(testset, threshold=4.0):\n",
    "    '''Return items considered relevant per user from the testset.'''\n",
    "    relevant = defaultdict(set)\n",
    "    for uid, iid, true_r in testset:\n",
    "        if true_r >= threshold:\n",
    "            relevant[uid].add(iid)\n",
    "    return relevant"
   ],
   "id": "47d75065d11bbd01",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:27.664525Z",
     "start_time": "2025-05-16T20:42:27.658772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def precision_at_k(top_k_preds, relevant_items, k):\n",
    "    precisions = []\n",
    "    for uid in top_k_preds:\n",
    "        if uid in relevant_items:\n",
    "            hits = len(set(top_k_preds[uid]) & relevant_items[uid])\n",
    "            precisions.append(hits / k)\n",
    "\n",
    "    if len(precisions) == 0:\n",
    "        print(\"Warning: No overlap between predicted users and relevant users.\")\n",
    "        return 0.0\n",
    "\n",
    "    return sum(precisions) / len(precisions)\n",
    "\n",
    "def hit_rate_at_k(top_k_preds, relevant_items):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    for uid in relevant_items:\n",
    "        total += 1\n",
    "        if set(top_k_preds[uid]) & relevant_items[uid]:\n",
    "            hits += 1\n",
    "\n",
    "    if hits == 0:\n",
    "        print(\"Warning: No overlap between predicted users and relevant users.\")\n",
    "        return 0.0\n",
    "    return hits / total"
   ],
   "id": "95cd57faca98aac9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reading data",
   "id": "9d44b5fd57d63000"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:29.636614Z",
     "start_time": "2025-05-16T20:42:27.714456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr"
   ],
   "id": "3c1ada155219e2da",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:30.160245Z",
     "start_time": "2025-05-16T20:42:29.659019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('../data/RecSys_AspectSentiment_train.csv')\n",
    "df_train.head()"
   ],
   "id": "f53ae225edcb17fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         username  beer_id  feel_predicted_rating  look_predicted_rating  \\\n",
       "0       Mitchster     2614                  3.960                  3.940   \n",
       "1        avalon07    13874                  4.270                  4.255   \n",
       "2          Kulrak     3999                  2.420                  2.410   \n",
       "3        jampics2    95343                  3.475                  3.425   \n",
       "4  lacqueredmouse    28314                  1.870                  1.870   \n",
       "\n",
       "   smell_predicted_rating  taste_predicted_rating  feel_true_rating  \\\n",
       "0                   3.940                   3.940             2.500   \n",
       "1                   4.260                   4.255             3.750   \n",
       "2                   2.420                   2.420             2.000   \n",
       "3                   3.425                   3.450             3.125   \n",
       "4                   1.890                   1.900             1.000   \n",
       "\n",
       "   look_true_rating  smell_true_rating  taste_true_rating        date  \\\n",
       "0             3.500              3.500              3.500  2005-02-18   \n",
       "1             4.125              3.625              3.625  2018-04-14   \n",
       "2             3.000              1.500              1.500  2007-05-28   \n",
       "3             3.750              3.125              2.750  2013-07-01   \n",
       "4             2.000              2.000              1.000  2006-11-04   \n",
       "\n",
       "                                                text  overall  score  \n",
       "0  recommended nick liquormax loveland. bottle co...     3.00   3.30  \n",
       "1  l: poured bottle pint glass. amber color cloud...     3.75   3.81  \n",
       "2  pours somewhat dark golden color crumbly white...     2.00   1.74  \n",
       "3  happy 25th great lakes! so, going hoppy lager ...     2.50   2.48  \n",
       "4  clear light yellow body, coarse-bubbled scummy...     1.50   1.40  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>feel_predicted_rating</th>\n",
       "      <th>look_predicted_rating</th>\n",
       "      <th>smell_predicted_rating</th>\n",
       "      <th>taste_predicted_rating</th>\n",
       "      <th>feel_true_rating</th>\n",
       "      <th>look_true_rating</th>\n",
       "      <th>smell_true_rating</th>\n",
       "      <th>taste_true_rating</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>overall</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mitchster</td>\n",
       "      <td>2614</td>\n",
       "      <td>3.960</td>\n",
       "      <td>3.940</td>\n",
       "      <td>3.940</td>\n",
       "      <td>3.940</td>\n",
       "      <td>2.500</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.500</td>\n",
       "      <td>2005-02-18</td>\n",
       "      <td>recommended nick liquormax loveland. bottle co...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avalon07</td>\n",
       "      <td>13874</td>\n",
       "      <td>4.270</td>\n",
       "      <td>4.255</td>\n",
       "      <td>4.260</td>\n",
       "      <td>4.255</td>\n",
       "      <td>3.750</td>\n",
       "      <td>4.125</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.625</td>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>l: poured bottle pint glass. amber color cloud...</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kulrak</td>\n",
       "      <td>3999</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.410</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.500</td>\n",
       "      <td>2007-05-28</td>\n",
       "      <td>pours somewhat dark golden color crumbly white...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jampics2</td>\n",
       "      <td>95343</td>\n",
       "      <td>3.475</td>\n",
       "      <td>3.425</td>\n",
       "      <td>3.425</td>\n",
       "      <td>3.450</td>\n",
       "      <td>3.125</td>\n",
       "      <td>3.750</td>\n",
       "      <td>3.125</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>happy 25th great lakes! so, going hoppy lager ...</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lacqueredmouse</td>\n",
       "      <td>28314</td>\n",
       "      <td>1.870</td>\n",
       "      <td>1.870</td>\n",
       "      <td>1.890</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2006-11-04</td>\n",
       "      <td>clear light yellow body, coarse-bubbled scummy...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:30.403539Z",
     "start_time": "2025-05-16T20:42:30.337735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = pd.read_csv('../data/RecSys_AspectSentiment_test.csv')\n",
    "df_test.head()"
   ],
   "id": "72478731c9560916",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       username  beer_id  feel_predicted_rating  look_predicted_rating  \\\n",
       "0         nokes    33201               3.940000               3.910000   \n",
       "1        albern    30918               2.590000               2.560000   \n",
       "2  Amish_Ambush    88427               2.880000               2.860000   \n",
       "3      eberkman       65               2.385818               2.378909   \n",
       "4      TechMyst     1708               4.037059               4.023529   \n",
       "\n",
       "   smell_predicted_rating  taste_predicted_rating  feel_true_rating  \\\n",
       "0                3.940000                3.960000          4.000000   \n",
       "1                2.630000                2.620000          2.000000   \n",
       "2                2.850000                2.860000          3.250000   \n",
       "3                2.388000                2.390909          2.195455   \n",
       "4                4.027941                4.032353          4.500000   \n",
       "\n",
       "   look_true_rating  smell_true_rating  taste_true_rating        date  \\\n",
       "0          3.500000           3.500000           4.000000  2009-09-07   \n",
       "1          3.000000           2.500000           1.000000  2008-07-13   \n",
       "2          3.000000           3.250000           2.500000  2013-01-21   \n",
       "3          2.409091           1.963636           2.040909  2007-07-31   \n",
       "4          4.507353           4.426471           4.522059  2006-10-04   \n",
       "\n",
       "                                                text  overall  score  \n",
       "0  went saskatchewan roughriders hefe glass, fill...     4.00   3.85  \n",
       "1  whoa, pretty bad beer. pours pale yellow tons ...     1.50   1.68  \n",
       "2  sam adams spring variety pack? maple pecan por...     2.75   2.84  \n",
       "3  one buddy left one fridge came bunch stuff wan...     2.50   2.30  \n",
       "4  container volume: 750ml container type: bottle...     3.50   3.46  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>feel_predicted_rating</th>\n",
       "      <th>look_predicted_rating</th>\n",
       "      <th>smell_predicted_rating</th>\n",
       "      <th>taste_predicted_rating</th>\n",
       "      <th>feel_true_rating</th>\n",
       "      <th>look_true_rating</th>\n",
       "      <th>smell_true_rating</th>\n",
       "      <th>taste_true_rating</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>overall</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nokes</td>\n",
       "      <td>33201</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>3.910000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2009-09-07</td>\n",
       "      <td>went saskatchewan roughriders hefe glass, fill...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albern</td>\n",
       "      <td>30918</td>\n",
       "      <td>2.590000</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>2.630000</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2008-07-13</td>\n",
       "      <td>whoa, pretty bad beer. pours pale yellow tons ...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amish_Ambush</td>\n",
       "      <td>88427</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>sam adams spring variety pack? maple pecan por...</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eberkman</td>\n",
       "      <td>65</td>\n",
       "      <td>2.385818</td>\n",
       "      <td>2.378909</td>\n",
       "      <td>2.388000</td>\n",
       "      <td>2.390909</td>\n",
       "      <td>2.195455</td>\n",
       "      <td>2.409091</td>\n",
       "      <td>1.963636</td>\n",
       "      <td>2.040909</td>\n",
       "      <td>2007-07-31</td>\n",
       "      <td>one buddy left one fridge came bunch stuff wan...</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TechMyst</td>\n",
       "      <td>1708</td>\n",
       "      <td>4.037059</td>\n",
       "      <td>4.023529</td>\n",
       "      <td>4.027941</td>\n",
       "      <td>4.032353</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.507353</td>\n",
       "      <td>4.426471</td>\n",
       "      <td>4.522059</td>\n",
       "      <td>2006-10-04</td>\n",
       "      <td>container volume: 750ml container type: bottle...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:35.001469Z",
     "start_time": "2025-05-16T20:42:30.532966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "aspects = ['feel', 'look', 'smell', 'taste']\n",
    "\n",
    "def cprime_from_arrays(tr, pr):\n",
    "    if tr.std() == 0 or pr.std() == 0:\n",
    "        dist = np.linalg.norm(tr - pr)\n",
    "        return 1.0 / (1.0 + dist)\n",
    "    corr = pearsonr(tr, pr)[0]\n",
    "    return (corr + 1.0) / 2.0\n",
    "\n",
    "for asp in aspects:\n",
    "    tcol = f'{asp}_true_rating'\n",
    "    pcol = f'{asp}_predicted_rating'\n",
    "    ccol = f'{asp}_C_prime'\n",
    "    wcol = f'{asp}_w'\n",
    "    rcol = f'{asp}_r_star'\n",
    "\n",
    "    # Compute C' per user from training set\n",
    "    c_per_user = df_train.groupby('username').apply(\n",
    "        lambda g: cprime_from_arrays(g[tcol].values, g[pcol].values)\n",
    "    )\n",
    "\n",
    "    # Broadcast C' to train and test\n",
    "    global_mean = c_per_user.mean()\n",
    "    df_train[ccol] = df_train['username'].map(c_per_user)\n",
    "    df_test[ccol] = df_test['username'].map(c_per_user).fillna(global_mean)\n",
    "\n",
    "    # Compute weights\n",
    "    df_train[wcol] = 1.0 - df_train[ccol]\n",
    "    df_test[wcol] = 1.0 - df_test[ccol]\n",
    "\n",
    "    # Fuse ratings\n",
    "    df_train[rcol] = df_train[wcol] * df_train[pcol] + (1.0 - df_train[wcol]) * df_train[tcol]\n",
    "    df_test[rcol] = df_test[wcol] * df_test[pcol] + (1.0 - df_test[wcol]) * df_test[tcol]\n",
    "\n",
    "# Keep only username, beer_id, and r_star columns\n",
    "keep = ['username', 'beer_id'] + [f'{asp}_r_star' for asp in aspects]\n",
    "df_train = df_train[keep]\n",
    "df_test = df_test[keep]\n"
   ],
   "id": "33add02a0b83d48c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_859871/3955676713.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  c_per_user = df_train.groupby('username').apply(\n",
      "/tmp/ipykernel_859871/3955676713.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  c_per_user = df_train.groupby('username').apply(\n",
      "/tmp/ipykernel_859871/3955676713.py:10: NearConstantInputWarning: An input array is nearly constant; the computed correlation coefficient may be inaccurate.\n",
      "  corr = pearsonr(tr, pr)[0]\n",
      "/tmp/ipykernel_859871/3955676713.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  c_per_user = df_train.groupby('username').apply(\n",
      "/tmp/ipykernel_859871/3955676713.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  c_per_user = df_train.groupby('username').apply(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "592f389a-ac91-42db-ab17-643c7bf9bff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:35.087995Z",
     "start_time": "2025-05-16T20:42:35.062587Z"
    }
   },
   "source": [
    "df_train"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             username  beer_id  feel_r_star  look_r_star  smell_r_star  \\\n",
       "0           Mitchster     2614     2.820137     3.587638      3.548359   \n",
       "1            avalon07    13874     3.872023     4.134488      3.705450   \n",
       "2              Kulrak     3999     2.000000     3.000000      1.500000   \n",
       "3            jampics2    95343     3.169182     3.696930      3.176384   \n",
       "4      lacqueredmouse    28314     1.103457     1.980947      1.987130   \n",
       "...               ...      ...          ...          ...           ...   \n",
       "15518    callmemickey    57269     3.119777     3.494384      3.472037   \n",
       "15519    cvstrickland     6549     3.831853     4.466076      3.975974   \n",
       "15520             MJR     1520     3.018367     3.248957      3.485454   \n",
       "15521     BeerBelcher    34688     2.768127     2.990571      2.637901   \n",
       "15522     mikesgroove     8787     3.991394     3.987871      3.520616   \n",
       "\n",
       "       taste_r_star  \n",
       "0          3.571321  \n",
       "1          3.750585  \n",
       "2          1.500000  \n",
       "3          2.873151  \n",
       "4          1.097399  \n",
       "...             ...  \n",
       "15518      2.501220  \n",
       "15519      3.666978  \n",
       "15520      3.466515  \n",
       "15521      2.434037  \n",
       "15522      3.990010  \n",
       "\n",
       "[15523 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>feel_r_star</th>\n",
       "      <th>look_r_star</th>\n",
       "      <th>smell_r_star</th>\n",
       "      <th>taste_r_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mitchster</td>\n",
       "      <td>2614</td>\n",
       "      <td>2.820137</td>\n",
       "      <td>3.587638</td>\n",
       "      <td>3.548359</td>\n",
       "      <td>3.571321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avalon07</td>\n",
       "      <td>13874</td>\n",
       "      <td>3.872023</td>\n",
       "      <td>4.134488</td>\n",
       "      <td>3.705450</td>\n",
       "      <td>3.750585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kulrak</td>\n",
       "      <td>3999</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jampics2</td>\n",
       "      <td>95343</td>\n",
       "      <td>3.169182</td>\n",
       "      <td>3.696930</td>\n",
       "      <td>3.176384</td>\n",
       "      <td>2.873151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lacqueredmouse</td>\n",
       "      <td>28314</td>\n",
       "      <td>1.103457</td>\n",
       "      <td>1.980947</td>\n",
       "      <td>1.987130</td>\n",
       "      <td>1.097399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15518</th>\n",
       "      <td>callmemickey</td>\n",
       "      <td>57269</td>\n",
       "      <td>3.119777</td>\n",
       "      <td>3.494384</td>\n",
       "      <td>3.472037</td>\n",
       "      <td>2.501220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15519</th>\n",
       "      <td>cvstrickland</td>\n",
       "      <td>6549</td>\n",
       "      <td>3.831853</td>\n",
       "      <td>4.466076</td>\n",
       "      <td>3.975974</td>\n",
       "      <td>3.666978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15520</th>\n",
       "      <td>MJR</td>\n",
       "      <td>1520</td>\n",
       "      <td>3.018367</td>\n",
       "      <td>3.248957</td>\n",
       "      <td>3.485454</td>\n",
       "      <td>3.466515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15521</th>\n",
       "      <td>BeerBelcher</td>\n",
       "      <td>34688</td>\n",
       "      <td>2.768127</td>\n",
       "      <td>2.990571</td>\n",
       "      <td>2.637901</td>\n",
       "      <td>2.434037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15522</th>\n",
       "      <td>mikesgroove</td>\n",
       "      <td>8787</td>\n",
       "      <td>3.991394</td>\n",
       "      <td>3.987871</td>\n",
       "      <td>3.520616</td>\n",
       "      <td>3.990010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15523 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "75f7f138-5ff8-4adc-b769-c026051e8e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:35.171842Z",
     "start_time": "2025-05-16T20:42:35.161552Z"
    }
   },
   "source": [
    "df_test"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          username  beer_id  feel_r_star  look_r_star  smell_r_star  \\\n",
       "0            nokes    33201     3.986460     3.592514      3.597048   \n",
       "1           albern    30918     2.150349     2.981730      2.515177   \n",
       "2     Amish_Ambush    88427     3.166506     2.968410      3.161774   \n",
       "3         eberkman       65     2.236367     2.401736      1.969650   \n",
       "4         TechMyst     1708     4.429263     4.392458      4.334418   \n",
       "...            ...      ...          ...          ...           ...   \n",
       "3876       Brolo75     7971     4.694140     4.580919      4.685124   \n",
       "3877     HopsYeast    63860     3.831237     3.922667      3.580591   \n",
       "3878       Jadjunk     1381     2.898615     3.408347      2.741817   \n",
       "3879        Nuke77    17112     4.199381     4.201983      4.495073   \n",
       "3880      hardy008    31503     3.070995     3.012676      2.672179   \n",
       "\n",
       "      taste_r_star  \n",
       "0         3.990434  \n",
       "1         1.258316  \n",
       "2         2.586090  \n",
       "3         2.118668  \n",
       "4         4.384478  \n",
       "...            ...  \n",
       "3876      4.747996  \n",
       "3877      3.832503  \n",
       "3878      2.626071  \n",
       "3879      4.294432  \n",
       "3880      2.869309  \n",
       "\n",
       "[3881 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>feel_r_star</th>\n",
       "      <th>look_r_star</th>\n",
       "      <th>smell_r_star</th>\n",
       "      <th>taste_r_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nokes</td>\n",
       "      <td>33201</td>\n",
       "      <td>3.986460</td>\n",
       "      <td>3.592514</td>\n",
       "      <td>3.597048</td>\n",
       "      <td>3.990434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albern</td>\n",
       "      <td>30918</td>\n",
       "      <td>2.150349</td>\n",
       "      <td>2.981730</td>\n",
       "      <td>2.515177</td>\n",
       "      <td>1.258316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amish_Ambush</td>\n",
       "      <td>88427</td>\n",
       "      <td>3.166506</td>\n",
       "      <td>2.968410</td>\n",
       "      <td>3.161774</td>\n",
       "      <td>2.586090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eberkman</td>\n",
       "      <td>65</td>\n",
       "      <td>2.236367</td>\n",
       "      <td>2.401736</td>\n",
       "      <td>1.969650</td>\n",
       "      <td>2.118668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TechMyst</td>\n",
       "      <td>1708</td>\n",
       "      <td>4.429263</td>\n",
       "      <td>4.392458</td>\n",
       "      <td>4.334418</td>\n",
       "      <td>4.384478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>Brolo75</td>\n",
       "      <td>7971</td>\n",
       "      <td>4.694140</td>\n",
       "      <td>4.580919</td>\n",
       "      <td>4.685124</td>\n",
       "      <td>4.747996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>HopsYeast</td>\n",
       "      <td>63860</td>\n",
       "      <td>3.831237</td>\n",
       "      <td>3.922667</td>\n",
       "      <td>3.580591</td>\n",
       "      <td>3.832503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>Jadjunk</td>\n",
       "      <td>1381</td>\n",
       "      <td>2.898615</td>\n",
       "      <td>3.408347</td>\n",
       "      <td>2.741817</td>\n",
       "      <td>2.626071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>Nuke77</td>\n",
       "      <td>17112</td>\n",
       "      <td>4.199381</td>\n",
       "      <td>4.201983</td>\n",
       "      <td>4.495073</td>\n",
       "      <td>4.294432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>hardy008</td>\n",
       "      <td>31503</td>\n",
       "      <td>3.070995</td>\n",
       "      <td>3.012676</td>\n",
       "      <td>2.672179</td>\n",
       "      <td>2.869309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3881 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "000582b5-031e-4d26-894e-afb23b0c2067",
   "metadata": {},
   "source": [
    "# item-based-collaborative-filtering"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5df6829-ebae-47ac-a84d-162b681e7c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:42:35.420646Z",
     "start_time": "2025-05-16T20:42:35.383230Z"
    }
   },
   "source": [
    "aspects = ['feel', 'look', 'smell', 'taste']\n",
    "df_train['overall_r_star'] = df_train[[f'{asp}_r_star' for asp in aspects]].mean(axis=1)\n",
    "df_test['overall_r_star'] = df_test[[f'{asp}_r_star' for asp in aspects]].mean(axis=1)\n",
    "df_train"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             username  beer_id  feel_r_star  look_r_star  smell_r_star  \\\n",
       "0           Mitchster     2614     2.820137     3.587638      3.548359   \n",
       "1            avalon07    13874     3.872023     4.134488      3.705450   \n",
       "2              Kulrak     3999     2.000000     3.000000      1.500000   \n",
       "3            jampics2    95343     3.169182     3.696930      3.176384   \n",
       "4      lacqueredmouse    28314     1.103457     1.980947      1.987130   \n",
       "...               ...      ...          ...          ...           ...   \n",
       "15518    callmemickey    57269     3.119777     3.494384      3.472037   \n",
       "15519    cvstrickland     6549     3.831853     4.466076      3.975974   \n",
       "15520             MJR     1520     3.018367     3.248957      3.485454   \n",
       "15521     BeerBelcher    34688     2.768127     2.990571      2.637901   \n",
       "15522     mikesgroove     8787     3.991394     3.987871      3.520616   \n",
       "\n",
       "       taste_r_star  overall_r_star  \n",
       "0          3.571321        3.381864  \n",
       "1          3.750585        3.865636  \n",
       "2          1.500000        2.000000  \n",
       "3          2.873151        3.228912  \n",
       "4          1.097399        1.542233  \n",
       "...             ...             ...  \n",
       "15518      2.501220        3.146855  \n",
       "15519      3.666978        3.985220  \n",
       "15520      3.466515        3.304823  \n",
       "15521      2.434037        2.707659  \n",
       "15522      3.990010        3.872473  \n",
       "\n",
       "[15523 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>feel_r_star</th>\n",
       "      <th>look_r_star</th>\n",
       "      <th>smell_r_star</th>\n",
       "      <th>taste_r_star</th>\n",
       "      <th>overall_r_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mitchster</td>\n",
       "      <td>2614</td>\n",
       "      <td>2.820137</td>\n",
       "      <td>3.587638</td>\n",
       "      <td>3.548359</td>\n",
       "      <td>3.571321</td>\n",
       "      <td>3.381864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avalon07</td>\n",
       "      <td>13874</td>\n",
       "      <td>3.872023</td>\n",
       "      <td>4.134488</td>\n",
       "      <td>3.705450</td>\n",
       "      <td>3.750585</td>\n",
       "      <td>3.865636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kulrak</td>\n",
       "      <td>3999</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jampics2</td>\n",
       "      <td>95343</td>\n",
       "      <td>3.169182</td>\n",
       "      <td>3.696930</td>\n",
       "      <td>3.176384</td>\n",
       "      <td>2.873151</td>\n",
       "      <td>3.228912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lacqueredmouse</td>\n",
       "      <td>28314</td>\n",
       "      <td>1.103457</td>\n",
       "      <td>1.980947</td>\n",
       "      <td>1.987130</td>\n",
       "      <td>1.097399</td>\n",
       "      <td>1.542233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15518</th>\n",
       "      <td>callmemickey</td>\n",
       "      <td>57269</td>\n",
       "      <td>3.119777</td>\n",
       "      <td>3.494384</td>\n",
       "      <td>3.472037</td>\n",
       "      <td>2.501220</td>\n",
       "      <td>3.146855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15519</th>\n",
       "      <td>cvstrickland</td>\n",
       "      <td>6549</td>\n",
       "      <td>3.831853</td>\n",
       "      <td>4.466076</td>\n",
       "      <td>3.975974</td>\n",
       "      <td>3.666978</td>\n",
       "      <td>3.985220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15520</th>\n",
       "      <td>MJR</td>\n",
       "      <td>1520</td>\n",
       "      <td>3.018367</td>\n",
       "      <td>3.248957</td>\n",
       "      <td>3.485454</td>\n",
       "      <td>3.466515</td>\n",
       "      <td>3.304823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15521</th>\n",
       "      <td>BeerBelcher</td>\n",
       "      <td>34688</td>\n",
       "      <td>2.768127</td>\n",
       "      <td>2.990571</td>\n",
       "      <td>2.637901</td>\n",
       "      <td>2.434037</td>\n",
       "      <td>2.707659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15522</th>\n",
       "      <td>mikesgroove</td>\n",
       "      <td>8787</td>\n",
       "      <td>3.991394</td>\n",
       "      <td>3.987871</td>\n",
       "      <td>3.520616</td>\n",
       "      <td>3.990010</td>\n",
       "      <td>3.872473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15523 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "609fdd3f-1864-4cd3-a49b-701c6b83d177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:43:31.956507Z",
     "start_time": "2025-05-16T20:42:35.680844Z"
    }
   },
   "source": [
    "from surprise import Dataset, Reader, KNNBaseline\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Reader + load Dataset\n",
    "reader = Reader(rating_scale=(df_train['overall_r_star'].min(),\n",
    "                              df_train['overall_r_star'].max()))\n",
    "data = Dataset.load_from_df(\n",
    "    df_train[['username', 'beer_id', 'overall_r_star']],\n",
    "    reader\n",
    ")\n",
    "\n",
    "# GridSearchCV for tuning\n",
    "param_grid = {\n",
    "    'k': [5, 10, 20, 40],\n",
    "    'min_k': [1, 2, 5],\n",
    "    'sim_options': {\n",
    "        'name': ['pearson', 'cosine'],\n",
    "        'user_based': [False]\n",
    "    }\n",
    "}\n",
    "gs = GridSearchCV(\n",
    "    KNNBaseline,\n",
    "    param_grid,\n",
    "    measures=['rmse'],\n",
    "    cv=3,\n",
    "    n_jobs=1\n",
    ")\n",
    "gs.fit(data)\n",
    "\n",
    "print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "print(\"Best parameters:\", gs.best_params['rmse'])\n",
    "\n",
    "# Build final trainset and model\n",
    "trainset = data.build_full_trainset()\n",
    "best_opts = gs.best_params['rmse']\n",
    "algo = KNNBaseline(\n",
    "    k=best_opts['k'],\n",
    "    min_k=best_opts['min_k'],\n",
    "    sim_options=best_opts['sim_options']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Prepare test set\n",
    "testset = list(\n",
    "    df_test[['username', 'beer_id', 'overall_r_star']]\n",
    "    .itertuples(index=False, name=None)\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = algo.test(testset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Best RMSE score: 0.5918633487732318\n",
      "Best parameters: {'k': 5, 'min_k': 1, 'sim_options': {'name': 'pearson', 'user_based': False}}\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "6e698c93-ee16-458c-81eb-7888a862e84f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:43:32.543171Z",
     "start_time": "2025-05-16T20:43:32.055380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from surprise.accuracy import mse, rmse, mae\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rmse_score = rmse(predictions, verbose=False)\n",
    "mae_score  = mae(predictions, verbose=False)\n",
    "mse_score  = mse(predictions, verbose=False)\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'MSE']\n",
    "scores  = [rmse_score, mae_score, mse_score]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(metrics, scores)\n",
    "\n",
    "for i, v in enumerate(scores):\n",
    "    ax.text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title(\"Test Set Performance\")\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ],
   "id": "77f1f68b44c2342e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:43:32.575393Z",
     "start_time": "2025-05-16T20:43:32.567030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "K = 10\n",
    "top_k_preds = get_top_k(predictions, k=K)\n",
    "relevant_items = get_true_positives(testset, threshold=4.0)\n",
    "\n",
    "prec = precision_at_k(top_k_preds, relevant_items, k=K)\n",
    "hit = hit_rate_at_k(top_k_preds, relevant_items)\n",
    "\n",
    "print(f'Precision@{K}: {prec:.4f}')\n",
    "print(f'Hit Rate@{K}: {hit:.4f}')"
   ],
   "id": "791d6cfb2194feea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.1274\n",
      "Hit Rate@10: 0.9988\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:43:32.853597Z",
     "start_time": "2025-05-16T20:43:32.609909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "def recommend_user(algo, trainset, user_id, N=5):\n",
    "    \"\"\"\n",
    "    Returns a list of (item_id, estimated_score) for the top-N\n",
    "    items the user has not yet rated.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inner_uid = trainset.to_inner_uid(user_id)\n",
    "    except ValueError:\n",
    "        return []  # user not in trainset\n",
    "\n",
    "    # items the user has already rated\n",
    "    seen_iids = {iid for (iid, _) in trainset.ur[inner_uid]}\n",
    "    # map all internal iids back to raw ids\n",
    "    raw_iids = [trainset.to_raw_iid(i) for i in trainset.all_items()]\n",
    "\n",
    "    # candidates = those the user hasn't seen\n",
    "    candidates = [iid for iid in raw_iids if trainset.to_inner_iid(iid) not in seen_iids]\n",
    "\n",
    "    # predict each candidate\n",
    "    scores = [(iid, algo.predict(user_id, iid).est) for iid in candidates]\n",
    "    # sort by score descending and take top N\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:N]\n",
    "\n",
    "for user in ['Boilermaker88', 'budgood1', 'draheim']:\n",
    "    top5 = recommend_user(algo, trainset, user, N=5)\n",
    "    print(f\"\\nTop-5 recommendations for {user}:\")\n",
    "    for beer_id, score in top5:\n",
    "        print(f\"  Beer {beer_id}: {score:.2f}\")"
   ],
   "id": "7dc99780610261c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 recommendations for Boilermaker88:\n",
      "  Beer 7971: 4.20\n",
      "  Beer 1545: 4.18\n",
      "  Beer 11757: 4.15\n",
      "  Beer 19960: 4.14\n",
      "  Beer 16814: 4.10\n",
      "\n",
      "Top-5 recommendations for budgood1:\n",
      "  Beer 7971: 4.06\n",
      "  Beer 1545: 4.04\n",
      "  Beer 11757: 4.01\n",
      "  Beer 19960: 4.00\n",
      "  Beer 16814: 3.96\n",
      "\n",
      "Top-5 recommendations for draheim:\n",
      "  Beer 7971: 4.26\n",
      "  Beer 1545: 4.24\n",
      "  Beer 11757: 4.20\n",
      "  Beer 19960: 4.19\n",
      "  Beer 16814: 4.15\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "3f8aae20-570f-41fb-b68e-0873076bd1ef",
   "metadata": {},
   "source": [
    "# Comparison Summary: Item-Based KNNBaseline With vs Without Sentiment Score\n",
    "\n",
    "The item-based KNNBaseline model with sentiment score (overall_r_star) shows a clear improvement over the version without sentiment:\n",
    "\n",
    "- RMSE decreased from 0.81 to 0.56\n",
    "- MAE decreased from 0.67 to 0.46  \n",
    "- MSE decreased from 0.66 to 0.31  \n",
    "\n",
    "These results indicate that incorporating aspect-level sentiment information significantly improves the model's ability to predict user preferences more accurately.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
